{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7cd924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import re\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout\n",
    "\n",
    "from kerastuner import HyperParameters\n",
    "from kerastuner.tuners import RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d289bcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('translated_dataset.csv')\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "056a6546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Indonesian stopwords and stemmer\n",
    "stop_factory = StopWordRemoverFactory()\n",
    "stopwords_sastrawi = set(stop_factory.get_stop_words())\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "custom_stopwords = {\n",
    "    # Pronouns & common personal references\n",
    "    'aku', 'kamu', 'dia', 'kami', 'kita', 'mereka', 'saya', 'anda', 'gw', 'gue', 'lu', 'loe', 'lo', 'gua', 'kak',\n",
    "    \n",
    "    # Formal stopwords\n",
    "    'yang', 'dan', 'di', 'ke', 'dari', 'pada', 'dengan', 'untuk', 'dalam', 'adalah', 'itu', 'ini',\n",
    "    'juga', 'sebagai', 'oleh', 'karena', 'agar', 'atau', 'tetapi', 'namun', 'sehingga', 'supaya',\n",
    "    'bagi', 'tanpa', 'bahwa', 'kalau', 'jika', 'saat', 'sebelum', 'sesudah', 'setelah', 'sementara',\n",
    "    'sambil', 'sedangkan', 'seperti', 'hingga', 'meskipun',\n",
    "\n",
    "    # Common verbs & auxiliaries\n",
    "    'adalah', 'ialah', 'merupakan', 'bukan', 'sudah', 'belum', 'akan', 'lagi', 'pernah',\n",
    "    'harus', 'mau', 'dapat', 'bisa',\n",
    "\n",
    "    # Informal abbreviations/slang\n",
    "    'gak', 'ga', 'nggak', 'ngga', 'tdk', 'ngerti', 'sih', 'aja', 'kok', 'dong', 'nih', 'deh',\n",
    "    'ya', 'yah', 'yee', 'loh', 'lah', 'cie', 'eh', 'btw', 'kalo', 'kalau', 'tp', 'yg', 'dr', 'pd', 'trs', 'klo'\n",
    "\n",
    "    # Internet filler\n",
    "    'wkwk', 'wkwkwk', 'haha', 'hehe', 'huhu', 'ckck', 'lol', 'lmao', 'anjay', 'banget', 'mantul',\n",
    "\n",
    "    # Temporal\n",
    "    'kemarin', 'besok', 'nanti', 'sekarang', 'tadi', 'barusan',\n",
    "\n",
    "    # Common question words\n",
    "    'apa', 'kenapa', 'mengapa', 'dimana', 'kapan', 'siapa', 'bagaimana'\n",
    "}\n",
    "\n",
    "extended_stopwords = stopwords_sastrawi.union(custom_stopwords)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    text = \" \".join([word for word in text.split() if word not in extended_stopwords])\n",
    "    text = stemmer.stem(text)  # stemming in Bahasa Indonesia\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b15efc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_column = 'text'\n",
    "label_column = 'label'\n",
    "\n",
    "df = df[[text_column, label_column]].dropna()\n",
    "df[text_column] = df[text_column].astype(str).apply(preprocess_text)\n",
    "\n",
    "if df[label_column].dtype != int:\n",
    "    le = LabelEncoder()\n",
    "    df[label_column] = le.fit_transform(df[label_column])\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb1bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_texts = df[df[label_column] == 1][text_column]\n",
    "negative_texts = df[df[label_column] == 0][text_column]\n",
    "\n",
    "positive_wc = WordCloud(width=800, height=400, background_color='white', colormap='Greens').generate(' '.join(positive_texts))\n",
    "negative_wc = WordCloud(width=800, height=400, background_color='white', colormap='Reds').generate(' '.join(negative_texts))\n",
    "\n",
    "plt.figure(figsize=(16, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(positive_wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Positive Words')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(negative_wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Negative Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3058318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "max_len = 100\n",
    "embedding_dim = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(df[text_column])\n",
    "sequences = tokenizer.texts_to_sequences(df[text_column])\n",
    "padded = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49084200",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index = {}\n",
    "embedding_dim = 300  # FastText Indo = 300 dimensions\n",
    "\n",
    "with open('cc.id.300.vec', encoding='utf-8') as f:\n",
    "    next(f)  # Skip the header\n",
    "    for line in f:\n",
    "        values = line.rstrip().split(' ')\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = vector\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < vocab_size:\n",
    "        vector = embedding_index.get(word)\n",
    "        if vector is not None:\n",
    "            embedding_matrix[i] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37765d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    padded, df[label_column], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a65c38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size,\n",
    "              output_dim=embedding_dim,\n",
    "              weights=[embedding_matrix],\n",
    "              input_length=max_len,\n",
    "              trainable=True),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Dropout(0.5),\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.build(input_shape=(None, max_len))\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17167f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=15, batch_size=32,\n",
    "                    validation_data=(X_test, y_test), callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33810aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b26f50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29d092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('sentiment_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
